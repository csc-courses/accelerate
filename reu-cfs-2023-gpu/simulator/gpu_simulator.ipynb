{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba3ea87",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## GPU Simulator\n",
    "\n",
    "Burton Rosenberg\n",
    "_30 May 2023_\n",
    "\n",
    "_last update: 2 June 2023_\n",
    "\n",
    "-----\n",
    "\n",
    "This code is a framework for testing and demonstrating GPU algorithms. The framework\n",
    "defines a GPU class contaning two dictionaries. \n",
    "\n",
    "- One represents arrays defined in global memory, and is a mapping from strings to ndarrays. They are roughly analogous to the handles returned by CudaMalloc and used by CudaMemcpy and the parameters to kernels.\n",
    "- The other represents loaded kernels, and is a mapping from strings to functions. Because these kernels are defined outside the lexical scope in which they will be run, the arrays in global memory, and other kernels, are referenced by the strings known to the two dictionaries.\n",
    "\n",
    "A launch method is analogous to a CUDA launch, and invokes a grid of kernels including a context object that provides the thread index for the kernel instance.\n",
    "\n",
    "The following implementations of simple GPU algorithms gives the patter on use.\n",
    "\n",
    "- A GPU object is instantiated.\n",
    "- Kernels are defined and sent to the GPU object.\n",
    "- NdArrays are created and sent to the GPU object.\n",
    "- The launch method is invoked.\n",
    "- The GPU NdArrays are read for the results.\n",
    "\n",
    "As of this writing, there is no simulation of blocks and grids, and the threads are in a one-dimensional array. As there are no blocks, there are no synchronization primitives or shared memory. And obviously, there are no warps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3ab455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPU:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mem = {}\n",
    "        self.ker = {}\n",
    "        \n",
    "    def addKernel(self,name,kernel):\n",
    "        self.ker[name] = kernel\n",
    "\n",
    "    def addMemory(self,name,ndarray):\n",
    "        self.mem[name] = ndarray\n",
    "\n",
    "    def launch(self,name,n_threads,args):       \n",
    "        for i in range(n_threads):\n",
    "            ctx = (self.mem,self.ker,i)\n",
    "            self.ker[name](ctx,args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae9a6b",
   "metadata": {},
   "source": [
    "### The dot product\n",
    "\n",
    "The dot product of two vectors is done in parallel with two kernels,\n",
    "\n",
    "- A straight forward component-wise multiplication\n",
    "- A sum of all elements using a log n depth tree\n",
    "\n",
    "Arranging the sum, this method uses a folding approach. Intuitively this method divides the array in half, and moves the top half over the bottom half, aligning the elements. The sum then updates the values in the lower half.\n",
    "\n",
    "The loop invariant is somthing the answer S is always the sum of the first i elements in the array. Start i at the length of the array and half it every interation, until i is one.\n",
    "\n",
    "<pre>\n",
    "\n",
    "initial array:\n",
    "\n",
    "   +---+---+---+---+---+---+---+---+\n",
    "   | 0   1   2   3   4   5   6   7 |\n",
    "   +---+---+---+---+---+---+---+---+\n",
    "\n",
    "fold in half and add\n",
    "\n",
    "   +---+---+---+---+\n",
    "   | 4   5   6   7 |\n",
    "   +---+---+---+---+\n",
    "   +---+---+---+---+\n",
    "+  | 0   1   2   3 |\n",
    "   +---+---+---+---+\n",
    "====================\n",
    "   +---+---+---+---+\n",
    "   | 4   6   8  10 |\n",
    "   +---+---+---+---+\n",
    "\n",
    "fold in half and add\n",
    "\n",
    "   +---+---+\n",
    "   | 8  10 |\n",
    "   +---+---+\n",
    "   +---+---+\n",
    "+  | 4   6 |\n",
    "   +---+---+\n",
    "====================\n",
    "   +---+---+\n",
    "   |12  16 |\n",
    "   +---+---+\n",
    "   \n",
    "one last time\n",
    "\n",
    "   +---+\n",
    "   |12 |\n",
    "   +---+\n",
    "   +---+\n",
    "+  |16 |\n",
    "   +---+\n",
    "====================\n",
    "   +---+\n",
    "   |28 |\n",
    "   +---+\n",
    "\n",
    "</pre>\n",
    "\n",
    "#### Warp considerations\n",
    "\n",
    "If we use the initial cells of the array, thread lauches are for consecutive thread ID's. We use all the threads in a warp (until we are under array size of 32).\n",
    "\n",
    "#### Memory considerations\n",
    "\n",
    "Each thread has exculsive access to the two memory cells it works with.\n",
    "\n",
    "#### Synchronization\n",
    "\n",
    "Using the default stream, enqueue a sequence of thread launchs according to\n",
    "the having blocksize.\n",
    "\n",
    "#### Efficiency.\n",
    "\n",
    "This has log n phases, each phase using n/2 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f14b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [ 2.  0.  0. 10. 10.  5.  1.  7. 13.  2.  5.  2.  6. 14.  2.  3.]\n",
      "b: [15. 11.  4.  8. 14. 14.  1.  9.  6.  2.  0. 11.  2.  3. 13.  8.]\n",
      "calculated: 592.0, actual: 592.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_dot_product(k):\n",
    "    \n",
    "    def mult_array(ctx,args):\n",
    "        (_m,_k,_tid) = ctx\n",
    "        (a, b) = map((lambda r: _m[r]), args)\n",
    "\n",
    "        a[_tid] *= b[_tid]\n",
    "\n",
    "    def fold_array(ctx,args):\n",
    "        (_m,_k,_tid), (a,k) = ctx, args\n",
    "        a = _m[a]\n",
    "\n",
    "        a[_tid] += a[_tid+k]\n",
    "\n",
    "    n = 2**k\n",
    "\n",
    "    gpu = GPU()\n",
    "    gpu.addKernel('mult_array',mult_array)\n",
    "    gpu.addKernel('fold_array',fold_array)\n",
    "\n",
    "    a = (np.random.randint(0,n,n)).astype(float)\n",
    "    b = (np.random.randint(0,n,n)).astype(float)\n",
    "\n",
    "    gpu.addMemory('a',a)\n",
    "    gpu.addMemory('b',b)\n",
    "    \n",
    "    print(f'a: {a}')\n",
    "    print(f'b: {b}')\n",
    "    d = a.dot(b)\n",
    "\n",
    "    gpu.launch('mult_array',n,('a','b'))\n",
    "    for i in range(k):\n",
    "        gpu.launch('fold_array',n//(2**(i+1)),('a',n//(2**(i+1))))\n",
    "    print(f'calculated: {a[0]}, actual: {d}')\n",
    "\n",
    "\n",
    "test_dot_product(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e672afc",
   "metadata": {},
   "source": [
    "### The partial sum\n",
    "\n",
    "The array is updated so a[i] contains the sum of the original values found in a[j] for all j less than or equal to i.\n",
    "\n",
    "The loop invariant is that blocks of size 2^k, starting of indices mutliples of 2^k, are correctly the partial sum array of just that block. Initially k=0 is satisfied trivially, and finally the k' for which 2^k'==n is the problem solved.\n",
    "\n",
    "The update from k to k+1 takes pairs of consecutive blocks of size 2^k and makes the loop invariant right for the combined block of size. 2^{k+1}. \n",
    "\n",
    "<pre>\n",
    "Initial array:\n",
    "\n",
    "   +---+---+---+---+---+---+---+---+\n",
    "   | 1   2   1   3 | 2   3   2   1 |   L.I. true for 2^0 = 1\n",
    "   +---+---+---+---+---+---+---+---+\n",
    "\n",
    "....\n",
    "\n",
    "   +---+---+---+---+---+---+---+---+\n",
    "   | 1   3   4   7 | 2   5   7   8 |   L.I true for 2^2 =4 \n",
    "   +---+---+---+---+---+---+---+---+\n",
    "                 |   |   |   |   |\n",
    "                 |   V   V   V   V\n",
    "                 +-&gt;   shift up\n",
    "                     |   |   |   |\n",
    "                     V   V   V   V\n",
    "   +---+---+---+---+---+---+---+---+\n",
    "   | 1   3   4   7 | 9  12  14  15 |   L.I true for 2^3 =8\n",
    "   +---+---+---+---+---+---+---+---+               \n",
    "</pre>\n",
    "\n",
    "\n",
    "#### Warp considerations\n",
    "\n",
    "Active threads are in consecutive thread ID's, so the warps are fully untilized \n",
    "except before the block size is 32. The code below launches n threads, but n/2\n",
    "threads are suffcient. \n",
    "\n",
    "#### Memory considerations\n",
    "\n",
    "Each thread has exclusive access to one location in the array and shared read access\n",
    "to another. Reads by multiple threads to the same location can be satisfied through\n",
    "caching and braoadcast.\n",
    "\n",
    "#### Synchronization\n",
    "\n",
    "Using the default stream, enqueue a sequence of thread launchs according to\n",
    "the having blocksize.\n",
    "\n",
    "#### Efficiency.\n",
    "\n",
    "This has log n phases, each phase using n/2 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6009c4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [ 3. 25.  1. 24. 10. 27. 30. 29.  0. 19.  8. 27.  3. 11.  2. 27. 29. 12.\n",
      " 18. 10. 22. 26. 10. 22.  0.  4. 30. 31.  3. 31. 10.  1.]\n",
      "a: [  3.  28.  29.  53.  63.  90. 120. 149. 149. 168. 176. 203. 206. 217.\n",
      " 219. 246. 275. 287. 305. 315. 337. 363. 373. 395. 395. 399. 429. 460.\n",
      " 463. 494. 504. 505.]\n",
      "error: 0.0\n"
     ]
    }
   ],
   "source": [
    "def test_partial_sum(k):\n",
    "    \n",
    "    # GPU KERNEL\n",
    "    \n",
    "    def raise_block(ctx,args):\n",
    "        (_m,_k,_tid), (a,k) =  ctx, args\n",
    "        a = _m[a]\n",
    "\n",
    "        t = _tid>>k\n",
    "        if (t%2)==1: \n",
    "            j = (t<<k)-1\n",
    "            a[_tid] += a[j]           \n",
    "\n",
    "    # test functionality\n",
    "    \n",
    "    def partial_sum_cpu(b):\n",
    "        for i in range(1,len(b)):\n",
    "            b[i] += b[i-1]\n",
    "            \n",
    "    def dist(a,b):\n",
    "        p = 0\n",
    "        for i in range(len(a)):\n",
    "            p = abs(a[i]-b[i])\n",
    "        return p\n",
    "            \n",
    "    # GPU simulation\n",
    "    \n",
    "    gpu = GPU()\n",
    "    gpu.addKernel('raise_block',raise_block)\n",
    "    \n",
    "    n = 2**k\n",
    "    a = (np.random.randint(0,n,n)).astype(float)\n",
    "    gpu.addMemory('a',a)\n",
    "    \n",
    "    b = a.copy()\n",
    "    partial_sum_cpu(b)\n",
    "    print(f'a: {a}')\n",
    "\n",
    "    for phase in range(k):\n",
    "        gpu.launch('raise_block',n,('a',phase))\n",
    "         \n",
    "    print(f'a: {a}')\n",
    "    print(f'error: {dist(a,b)}')\n",
    "    \n",
    "    \n",
    "test_partial_sum(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e027b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e870f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a2d3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
